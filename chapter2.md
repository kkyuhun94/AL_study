
## 2.마르코프 결정 프로세스(Markov Decision Process)

* 문제를 풀기 위해서는 문제가 잘 정의되어야 함
* 강화학습에서 문제를 잘 정의하려면 주어진 문제를 MDP(Markov Decision Process)의 형태로 만들어야 함

* 순차적 의사 결정 문제는 MDP라는 개념을 통해 더 정확하게 표현할 수 있음
* 가장 간단한 Markov Process를 이해하고, Markov Reward Process, 이후 Markov Decision Process를 이해하는 것이 목표
* 아주 중요한 내용.

### 2.1 Markov Process

#### 마르코프 프로세스(MP) : 미리 정의된 어떤 확률 분포를 따라서 상태와 상태 사이를 이동해 다니는 여정


#### S : 상태의 집합
	•	가능한 상태들을 모두 모아놓은 집합
	•	S = {s0,s1,s2,s3,s4}

#### P : 전이 확률 행렬
	•	전이 확률(transition probability)
	     •	상태 S에서 다음 상태 S’로 도착할 확률을 가리킴  
	•	조건부 확률로 표현가능
	     •	마르코프 프로세스는 정해진 간격으로 상태가 바뀜
	     •	시점 t에서의 상태 St가 s 일때
	     •	시점 t+1에서의 상태 St+1가 s’가 될 확률
	     •	전이 확률을 행렬의 형태로 표현


#### 마르코프 성질
	•	마르코프 프로세스는 모든 상태가 마르코프 성질(Markov property)를 따른다.
	•	마르코프 성질 : 미래는 오로지 현재에 의해 결정된다.
       •	St+1의 상태는 오로지 St에 의해서만 결정 된다.
	•	마르코프 상태의 예시
	     •	체스 : 현재 두어야 하는 최선의 수는 과거에 어떤 수를 두었는지와 관계가 없음
	     •	만약 현재 체스판의 사진이 있다면 그 사진만으로 최선의 수를 찾을 수 있음
	•	마르코프 하지 않은 상태의 예시
	     •	운전을 하고 있는 운전자의 상태
	     •	현재의 사진을 보더라도 후진을 하고 있는지, 속도가 어떤지, 정확하게 알수 없기 때문에 하나의 사진으로 구성한 상태는 마르코프하지 않음
	     •	최근 10초 동안의 사진 10장을 묶어서 상태로 제공한다면, 마르코프의 상태라고 볼 수 있음
	         •	앞으로 가고 있는지, 속도, 가속도 등을 파악 가능하기 때문
	•	마르코프한 상태도 있고, 마르코프 하지 않은 상태도 존재한다
	•	마르코프 프로세스로 모델링을 하기 위해서는 상태가 마르코프 해야하며, 단일 상태 정보로 충분한 정보를 파악할 수 있도록 상태를 구성해야함

#### 2.2 마르코프 리워드 프로세스(Markov Reward Process, MRP)

* MP에 보상의 개념을 추가

* 목표 : 잠드는 것. 많은 보상, 프로세스의 종료

#### MRP의 정의

•	상태의 집합 S
•	전이 확률 행렬 P
•	보상 함수 R
    •	어떤 상태 s에 도착했을 때 받게 되는 보상
    •	기대값을 사용하는 이유 : 특정 상태에 도달했을 때 받는 보상이 매번 조금씩 다를 수도 있음
    •	Ex. 보상을 받는 경우 10, 못받는 경우0 일때 기댓값은 5

•	감쇠 인자 r
    •	0과 1사이의 숫자
    •	미래 얻을 보상에 비해 당장 얻는 보상을 얼마나 더 중요하게 여길 것인지를 나타내는 파라미터
    •	미래에 얻을 보상의 값에 r가 여러번 곱해지며 그 값을 작게 만드는 역할
    •	현재부터 미래에 얻게 될 보상의 합 : 리턴(return)
    •	r는 왜필요 할까?
      •	똑같은 양의 보상일지라도 당장 받는 보상이 오랜시간 지난뒤 받는 보상보다 더 큰 가치를 가지를 가지게 하는 것.
      •	미래를 평가절하해주는 항
          •	극단적으로 r=0일 경우 : 미래의 보상이 모두 0이 되기 때문에 근시안적인 agent가 됨
          •	반대로 r=1일 경우 : 현재의 보상과 미래의 보상이 대등하기 때문에 장기적인 시야를 갖고 움직이는 agent가 되도록 함.
      •	수학적 편리성
          •	r를 1보다 작게 해줌으로써 리턴 Gt가 무한대의 값을 가지는 것을 방지
          •	무한한 값을 가지는 것을 방지함으로써 여러 이론들의 수학적 증명이 수월해짐
      •	사람의 선호 반영
          •	사람은 눈앞의 보상을 더 선호. 에이전트도 마찬가지의 개념을 도입
      •	미래에 대한 불확실성 반영
          •	약속을 지키지 못할 수도 있는 가능성을 배제할 수 없음
          •	현재와 미래 사이에 다양한 확률적 요소가 존재하고 이로 인해 당장 느끼는 가치에 비해 미래에 느끼는 가치가 달라질 수 있음. 이 불확실성을 반영하고자 감쇠 인자를 사용


#### 감쇠된 보상의 합, 리턴
	•	MRP에서는 MP와는 다르게 상태가 바뀔 때마다 해당하는 보상을 얻음
	    • 상태 s0에서 R0를 받고 시작하여 종료 상태인 sT에 도착할 때 보상 RT를 받으며 끝이남
	    •	이와 같은 하나의 여정을 강화학습에서는 에피소드(episode)라고 함
	•	에피소드
	•	리턴 : Gt
	    • t시점부터 미래에 받을 감쇠된 보상의 합
    	•	현재 타임 스텝이 t라면 그 이후에 발생하는 모든 보상의 값을 더해줌
    	•	현재에서 멀어질 수록, r가 더 많이 곱해져서 값이 0에 가까워짐
    	•	미래에 얻게 될 보상에 대한 가중치
    	•	강화학습에서 보상을 최대화 하도록 하는 것 x -> 보상의 합인 리턴을 최대화 하는것이 올바른 표현
    	•	리턴은 과거의 보상을 고려하지 않고 미래의 보상을 통해서 정의된다 !
    	•	에이전트의 목적 : 지금부터 미래에 받을 보상의 합인 Gt를 최대화 하는 것

#### MRP에서 각 상태의 밸류 평가하기
	•	어떤 상태의 가치(value)를 평가하고 싶다면 어떤 값을 사용해야 할까?
	•	현재부터 미래까지의 받을 보상들의 합, 리턴을 측정해서 가치를 판단
	   •	리턴의 값은 매번 바뀜 -> 리턴의 기댓값(Expectation)을 사용

#### 에피소드의 샘플링
	•	에피소드 : 시작 s0에서 출발하여 종료 상태 sT까지 가는 하나의 여정
	•	하나의 에피소드 안에서 방문하는 상태들은 매번 다름
  	  • 그에 따라 리턴이 달라짐
  	  •	강화 학습에서는 에비소드가 어떻게 샘플링 되느냐에 따라 리턴이 달라짐
  •	샘플링 : 표본 추출
    	•	어떤 확률 분포가 있을 때 해당 분포에서 샘플을 뽑아보는 것이 샘플링
    	•	많은 경우에 실제 확률 분포를 잘 모르는 경우가 대부분 -> 샘플링을 통해 어떤 값을 유추하는 방법론을 사용
    	•	Monte-Carlo 접근법 : 샘플링을 통해 어떤 확률 분포를 유추해서 사용하는 방법론
      •	전이 확률 P가 주어져 있다면 이런 에피소드 샘플들을 무한히 뽑아 낼 수 있다.

#### 상태 가치 함수(State Value Function)
	•	어떻게 하면 주어진 상태 s의 가치를 평가할 수 있을까?
	•	상태 가치 함수를 가정 : 상태를 인풋으로, 상태의 밸류를 아웃풋으로 출력하는 함수
	    •	에피소드 마다 리턴이 다르기 때문에 상태s의 value v(s)는 기댓값을 이용하여 정의

	•	상태 가치 함수 계산
    	•	시점 t에서 상태 s부터 시작해서 에피소드가 끝날 때 까지의 리턴을 계산
    	•	기댓값을 구하려면 에피소드별로 해당 에피소드가 발생할 확률과 그 때의 리턴 값을 곱해서 더해주어야 함
    	•	가능한 에피소드가 무한히 많기 때문에 이런 접근법은 현실적으로 불가능
    	•	그래서 샘플로 얻은 리턴의 평균을 통해 밸류의 근사값을 계산함

### 2.3 마르코프 결정 프로세스(Markov Decision Process)

* MP, MRP에서는 상태 변화가 자동으로 이루어짐 (다음 상태의 분포는 미리 정해져 있음)
* 여기에는 행동하는 주체가 존재 하지 않기 때문에 MP,MRP만으로 순차적 의사결정 문제의 모델링을 할 수 없음
* 의사 결정에 관한 부분이 모델에 포함되어야 함 -> 마르코프 결정 프로세스
* 행동의 주체인 에이전트가 등장

#### MDP의 정의
	•	MRP에 에이전트가 더해짐 / 에이전트는 각 상황마다 액션(행동)을 취함
	•	상태의 집합 S : MP, MRP에서의 S와 동일
	•	액션의 집합 A : 에이전트가 매 스텝마다 액션의 집합의 원소 중 하나를 선택함
    	•	전이 확률 행렬 P
    	•	현재 상태s에서 에이전트가 액션 a를 선택했을 때, 상태 s’이 될 확률
    	   •	상태 s에서 액션 a를 선택했을 때 도달하게 되는 상태가 결정론적이 아님
    	   •	같은 상태 s에서 액션 a를 선택해도 매번 다른 상태에 도착할 수 있음
	    •	조건부 확률 : 현재 상태 s에서, 액션 a를 선택 했을때
	•	보상 함수 R
    	•	MRP에서는 상태에 의해 보상이 정해짐
    	•	MDP에서는 액션이 추가 되었기 때문에 현재 상태 s에서 어떤 액션을 선택하느냐에 따라 받는 보상이 달라짐
    	•	보상함수 R : 상태 s에서 액션 a를 선택하면 받는 보상의 기댓값

	•	감쇠 인자 r
	    •	MRP에서의 감쇠인자와 동일

	•	아이가 잠드는 상황에 어머니라는 에이전트가 개입됨
	•	전이 확률이 100퍼인 경우 화살표를 생략한 그림

	•	아이의 상태는 그날 아이의 컨디션에 따라 바뀜. 환경의 일부. 에이전트가 조절할 수 있는 것이 아님
	•	MDP가 간단한 경우의 예시이므로 a0만 선택하는 것이 최적의 전략임을 쉽게 찾을 수 있음
	•	MDP가 복잡할 경우에는 좀 더 찾기 힘듬
	    •	상태 s에 따라 어떤 액션 a를 선택해야 보상의 합을 최대로 할 수 있는가? : 정책(policy)

#### 정책 함수와 2가지 가치 함수
	•	정책 함수(policy function)는 각 상태에서 어떤 액션을 선택할지 정해주는 함수
	•	Ex. 아이를 재우려는 어머니의 입장에서 아이의 상태에 따라 같이 놀아줄지, 자장가를 불러줄지 선택해야함
    	•	정책 : 놀아주기, 자장가 중 선택
    	•	목적은 보상의 합을 최대화하는 정책을 찾는것 -> 함수로 표현하면 정책함수

	•	각 상태에서 할 수 있는 모든 액션의 확률 값을 더하면 1이 되어야 함.
	•	하나의 액션에 100%의 확률이 몰려있는 것도 가능한 정책
	•	정책 : 액션 선택에 대한 운용방침.
	•	정책함수는 에이전트 안에 존대한다.
    	•	환경은 변하지 않지만 에이전트는 자신의 정책을 언제든 수정할 수 있다.
    	•	더 큰 보상을 얻기 위해 계속해서 정책을 교정해 나가는 것이 곧 강화학습

#### 상태 가치 함수(state value function)
	•	가치함수 : 어떤 상태를 평가하고 싶다는 의도에서 출발
	•	주어진 상태로부터 미래에 얻을 리턴의 기댓값 = 상태의 밸류.가치
	•	MDP에서는 에이전트의 액션이 추가되었기 때문에 에이전트의 정책함수에 따라서 얻는 리턴이 달라짐
	•	가치 함수를 정의하기 위해서 정책함수를 먼저 정의해야함
	•	MRP에서의 가치 함수


#### 액션 가치 함수
	•	각 상태에서의 액션도 평가할 수는 없을까?
	•	액션 가치 함수(state-action value function)
	•	상태 가치함수 : v(s)
	•	액션 가치함수 : q(s,a)
    	•	상태와 액션이 동시에 인풋으로 들어가야함
    	•	상태에 따라 액션의 결과값이 달라지기 때문


### 2.4 Prediction과 Control

#### 강화학습에서의 문제 세팅 : 주어진 상황을 MDP의 형태로 만들어서 풀고자 하는 것
	•	MDP (S,A,P,R,r)이 주어졌을 때 풀기위한 task 2가지
	1. Prediction : 정책함수(파이)가 주어졌을 때 각 상태의 밸류를 평가하는 문제
    	1. 에피소드 : 시작 부터 종료까지 발생 가능한 각각의 여정
    	2. 에피소드마다 얻을 수 있는 보상이 다르며 발생확률 또한 다름
    	3. 에피소드 리턴의 기댓값을 구하는 것
	2. Control : 최적 정책(파이*)를 찾는 문제
    	1.	최적의 정책 : 가장 기대 리턴이 큰 파이*
    	2.	최적의 정책 \pi 를 다를 때의 가치 함수를 최적가치함수(optimal value function)이라고 하며, \v로 표현함
    	3.	최적 가치 함수를 찾았을때 MDP를 풀었다고 말할 수 있음
